# -*- coding: utf-8 -*-
"""NLTKCorpus.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1cRXfq13XXpBHvnMPr6d7S8USnOjSmr8J

#Downloading all necessary Libraries
"""

!pip install PyPDF2

!pip install gensim

!pip install wordcloud

import PyPDF2
import os
from google.colab import drive
if os.path.exists('/content'):
  print("Yes!")
drive.mount('/content/drive')

"""#Imports"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import string
import gensim
from gensim import corpora
from pprint import pprint
from sklearn.decomposition import TruncatedSVD
from sklearn.feature_extraction.text import CountVectorizer
from nltk.stem import WordNetLemmatizer
from sklearn.feature_extraction.text import TfidfVectorizer
import re
import nltk
from nltk.corpus import stopwords #Stopwords is a dictionary of all the meaningless words in the corpus
from nltk.tokenize import word_tokenize
from wordcloud import WordCloud
from PIL import Image
from itertools import combinations
import networkx as nx
from sklearn.feature_extraction.text import TfidfVectorizer
plt.style.use("ggplot")

nltk.download('stopwords')
nltk.download('punkt')
nltk.download('wordnet')
nltk.download('vader_lexicon')

"""#Accessing the Files reports"""

dir_path = "/content/drive/MyDrive/MPR" #/content/drive/MyDrive/RBI_MPR, MPR, PKJ: /content/drive/MyDrive/PP

text = []

def extract_text1(file_path):
  text2 = ""
  with open(file_path, "rb") as file: #This opens the file at the specified path
    pdf_read = PyPDF2.PdfReader(file) #Used to read the file in pdf fromat
    for page_num in range(len(pdf_read.pages)): #.pages creates a list of pages in the file
      text2 += pdf_read.pages[page_num].extract_text() #Extracting the text from the given page
  return text2

for file_name in os.listdir(dir_path): #Iterates a newly made list of file_names present in the path given as argument to the function
  file_path = os.path.join(dir_path, file_name) #Joining files' names to the path of the directory given as dir_path
  print(dir_path, " - ",  file_path) #os.listdir returns the list of names of file in the directory
  if os.path.isfile(file_path) and file_name.endswith(".pdf"):
    #The code checks if the file is a regular file and if the name ends with .pdf
    text1 = extract_text1(file_path)
    text.append(text1)

len(text)

"""## Length Visualization of each doc"""

def plot_lens(list1, list2):
  x = np.arange(len(list2)) #Fixing integer x ticks
  plt.bar(x, list1)
  plt.title("Lenghts of documents by the year of publication")
  plt.xlabel("Reports")
  plt.ylabel("Character Counts")
  plt.xticks(x, list2, rotation = 270) #Mapping integer x ticks with the list2 values
  for i in range(len(list2)):
    plt.text(i, list1[i], str(list1[i]), ha='center', va='bottom')
  plt.show()
def format_names(dir_path):
  list1 = []
  for file_name in os.listdir(dir_path):
    list1.append(f"MPR_{file_name[5:-4]}_20{file_name[3:5]}")
  return list1

lens_list = []
MPR_names = []
for text3 in text:
  lens_list.append(len(text3))
  # print(len(text3))
MPR_names = format_names(dir_path)
plot_lens(lens_list, MPR_names)

MPR_names



"""# Text Pre processing"""

exclude = string.punctuation

print(exclude)

def remove_punc(text):
  return text.translate(str.maketrans('', '', exclude))

c_text1 = [] #c_text1 is the list of text after removal of the punctuation marks
for text3 in text:
  c_text1.append(remove_punc(text3))

for text3 in c_text1:
  print(len(text3))

#Removed all the punctuation marks from the text corpus

sdentence = 'Start a sentence and then bring it to an end '

pattern = re.compile(r'abc')
pattern

"""## Removing Numbers"""

c_text2 = []  #c_text2 is the list of text after numbers have been removed from the text
for text3 in c_text1:
  # t = re.sub(r"[\\"+-*/]", "", text3)
  t = re.sub(r'\d+', '', text3).lower()
  t = re.sub(r"\b\w\b", "", t)
  print(type(t))
  c_text2.append(t) #substitues all numbers by "" present in text3

def lenshower(list1):
  for i in list1:
    print(len(i))
    # print(i[123:232])
lenshower(c_text2)

#removed all the numbers from the corpus and also converted the resulting text to lower

"""## Removing Stopwords"""

def remove_sw(text1):
  new = []
  w = text1.split()
  for i in w:
    if i in stopwords.words("english"):
      new.append('')
    else:
      new.append(i)
  x = new[:]
  new.clear()
  return " ".join(x)

c_text3 = [] #c_text3 contains the lists of text after stopwords have been removed from the text
for text3 in c_text2:
  c_text3.append(remove_sw(text3))
lenshower(c_text3)

def remove_words(texts):
    # Define the words to be removed
    words_to_remove = ["per", "cent", "policy", "chart", "iv", "iii", "ii", "-"]
    pattern = r'\b(?:{})\b'.format('|'.join(map(re.escape, words_to_remove)))
    cleaned_texts = [re.sub(pattern, '', text, flags=re.IGNORECASE) for text in texts]

    return cleaned_texts


texts = ["This is a sample text with a per policy cent chart.",
         "The policy cent per chart is a visualization tool."]
c_text4 = remove_words(c_text3)
print(c_text4)
#The function removed some of the most used words from the passage that did not necessarily mean anything

"""# Analysis

## Generating Wordclouds
"""

def generate_word_clouds(text_list):
    for idx, text in enumerate(text_list):
        wordcloud = WordCloud(width=800, height=400, background_color='white').generate(text)
        plt.figure(figsize=(10, 5))
        plt.imshow(wordcloud, interpolation='bilinear')
        plt.axis('off')
        plt.title(f'Word Cloud {idx + 1}- {MPR_names[idx]}')
        plt.show()
generate_word_clouds(c_text4)

"""1. RBI reports of April 2022 has mentioned Crude Oil a good number of times, which has a direct relation to Russia Ukraine war, accelerated by the invasion of Russia, starting February, 2022. Crude oil was a hot topic in 2019 April report as well <br>
"**In 2018-19, India produced 34.20 million metric tonnes (MMT) of crude oil, with 71.15% coming from ONGC and OIL and 28.85% from private and joint venture companies. However, in 2019-2020, India's crude oil production fell to 32,173 TMT, the lowest level in at least 18 years**"
<br>
2. "Prices" is a frequent keyword for the April- October Period in 2021, on reseaarching a bit, it was found that India faced moderate inflation during that period, the primary reason for it can be the pandemic itself which reached its peak earlier in these months. The food prices witnessed a seious decline from 9.2% hike to about 2.9%. <br>
3. Some of the important keywords throughout the entire corpus of reports are:
["Price", "inflation", "Months", "Growth", "market", "demand", "rbi staff"]

## Generating tf-idf and tidytext
"""

def generate_tables(text_list):

    # TF-IDF calculation
    tfidf_vectorizer = TfidfVectorizer()
    tfidf_matrix = tfidf_vectorizer.fit_transform(text_list)
    tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=tfidf_vectorizer.get_feature_names_out())

    # Tidytext table
    tidytext_df = pd.DataFrame()
    tidytext_df['Word'] = tfidf_vectorizer.get_feature_names_out()
    for idx, text in enumerate(text_list):
        tidytext_df[f'Text {idx+1}'] = tfidf_df.iloc[idx].values

    return tfidf_df, tidytext_df

tfidf_table, tidytext_table = generate_tables(c_text4)
tidytext_table

tfidf_table

#All stopwords have been removed

"""## LSA"""

def apply_lsa(text_list):
    vectorizer = CountVectorizer()
    X = vectorizer.fit_transform(text_list)
    lsa = TruncatedSVD(n_components=20, random_state=42)
    lsa.fit(X)
    lsa_representation = lsa.transform(X)
    lsa_per_text = []
    for idx, text in enumerate(text_list):
        X_text = vectorizer.transform([text])
        lsa_text = lsa.transform(X_text)
        lsa_per_text.append(lsa_text)

    return lsa_representation, lsa_per_text
lsa_representation, lsa_per_text = apply_lsa(c_text3)

print("\nLSA representation for each text separately:")
for idx, lsa_text in enumerate(lsa_per_text):
    print(f"Text {idx + 1}: {lsa_text}")

from sklearn.metrics.pairwise import cosine_similarity

lsa_per_text = [
    [1295.85914841, -204.84934403, -125.18561655, -132.67910884, 37.37903193, 150.62077001, 50.17288184, 9.98601783, -29.75497524, 30.65132836, -3.30449942],
    [1035.85744262, -87.26580525, -61.33004077, -85.81985528, -26.16414584, -96.32821819, -6.13519336, 48.2899802, 11.41548096, -47.55663336, 97.40338196],
    [1115.61821649, -216.00132673, -18.28013556, 79.36641094, -4.86144939, -28.41545803, 12.18433882, -47.28394811, 89.09367088, -68.23705063, -52.08600839],
    [1041.96918983, 91.65883896, 201.39427194, -147.50881548, 61.98561561, -42.16964224, -12.02619868, -92.11726608, -66.79306397, -40.10588164, -16.3661447],
    [1003.6369738, 88.41789389, -51.97982791, -79.10034615, -95.65085104, -19.65565789, -104.82652893, 97.05305549, -14.2001766, -7.63357456, -64.97761291],
    [1123.65019064, -122.41602165, -33.60831141, 116.08514543, 22.02223339, -137.53804557, 21.07147562, -4.72688841, -55.72836642, 88.40209564, -14.71945983],
    [990.8506589, 13.95225832, 354.41176367, 35.09180892, 37.18474454, 43.75702933, 9.83123103, 68.03884788, 57.93039849, 38.65630577, 14.54686058],
    [971.73863673, 286.73344731, -90.17331997, -48.50384433, -92.89201489, -19.32313001, 122.72668548, -24.97004494, 49.94653293, 24.8994014, -5.55093913],
    [1007.28187, 25.6912072, 28.9290869, 149.785237, -157.890622, 90.117879, -64.5494646, -65.5696212, -33.9428168, -0.585044368, 38.7883042],
    [710.74560714, 139.38866667, -51.92623652, 167.13286482, 98.57736468, 34.79189096, 60.80363654, 64.95775517, -61.43992043, -72.43016848, -4.71117742],
    [820.88525293, 161.69219004, -157.81211351, 21.48162556, 162.20707124, 17.57751959, -104.02073473, -36.0535346, 55.70698039, 30.88906311, 18.9535786]
]

similarities = cosine_similarity(lsa_per_text)
sns.heatmap(similarities)
for i in range(len(similarities)):
    for j in range(len(similarities[0])):
        print(f"Similarity between Text {MPR_names[i]} and Text {MPR_names[j]}: {similarities[i][j]}")
for i in range(len(c_text4)):
  print(MPR_names[i], f" = text{i}")

"""High Variability in Magnitudes:

The magnitudes of the LSA representations vary significantly across the entire list and for each text separately.
This variability suggests differences in the importance of the latent semantic features captured by LSA for different texts.
Differences in Direction:

The signs of the values in the LSA representations indicate the direction of the semantic content along each dimension.
Texts with positive values along a dimension might emphasize certain semantic aspects, while those with negative values might emphasize different aspects.
Semantic Themes:

Similar LSA representations among texts might suggest shared semantic themes or topics.
For example, Texts 4 and 5 have similar representations, indicating potential similarity in their content.
Outliers and Distinctiveness:

Texts with LSA representations that significantly differ from the rest might represent outliers or texts with distinct semantic content.
Texts 1, 8, and 11 have relatively higher magnitudes in one or both dimensions, suggesting they might contain unique or distinctive semantic content compared to the others.
Clusters and Relationships:

Closer proximity in the reduced-dimensional space indicates semantic similarity between texts.
Texts with closer LSA representations are likely to share common semantic features or topics, while those farther apart may represent distinct topics or themes.
Dimension Interpretation:

While the exact interpretation of the dimensions in the reduced space might not be straightforward, they capture latent semantic features shared among the texts.
Further analysis or interpretation of these dimensions could provide deeper insights into the underlying semantic structure of the texts.

Insights from the LSA Analysis:
<br>
1. Same year reports seem to have higher similarity scores.
<br>
2. Reports from the same time of the year but cross years have decent to high sumilarity with each other suggesting towards a seasonal nature of the conversation and trend. <br>
3. Reports from April 2020 have lower similarity with other reports, citing obvious reasons, but the same is not oberved for October report of 2020, which still seem to have a higher similarities with other year reports. A similar pattern is observed for 2021 reports as well.
<br>
4. In recent yeaes, report in the middle of the year seem to have decent similarity with previous year's end reports

## Frequency Plots
"""

from collections import Counter

def plot_word_frequency(texts):
    all_text = ' '.join(texts)
    words = all_text.split()
    word_freq = Counter(words)
    sorted_word_freq = sorted(word_freq.items(), key=lambda x: x[1], reverse=True)
    top_words = [word[0] for word in sorted_word_freq[:40]]
    top_freqs = [word[1] for word in sorted_word_freq[:40]]
    plt.figure(figsize=(10, 6))
    plt.bar(range(len(top_words)), top_freqs, tick_label=top_words, color='skyblue')
    plt.xticks(rotation=45, ha='right')
    plt.xlabel('Words')
    plt.ylabel('Frequency')
    plt.title('Top 20 Most Frequent Words')
    plt.tight_layout()
    plt.show()
plot_word_frequency(c_text4)

def plot_histograms(texts):
    for i, text in enumerate(texts):
        words = text.split()
        word_freq = Counter(words)
        sorted_word_freq = sorted(word_freq.items(), key=lambda x: x[1], reverse=True)
        top_words = [word[0] for word in sorted_word_freq[:20]]
        top_freqs = [word[1] for word in sorted_word_freq[:20]]
        plt.figure(figsize=(10, 6))
        plt.bar(range(len(top_words)), top_freqs, tick_label=top_words, color='skyblue')
        plt.xticks(rotation=45, ha='right')
        plt.xlabel('Words')
        plt.ylabel('Frequency')
        plt.title(f'Text {MPR_names[i]} - Top 20 Most Frequent Words')
        plt.tight_layout()
        plt.show()
plot_histograms(c_text4)

lemmatizer = WordNetLemmatizer()
stop_words = set(stopwords.words('english'))

def preprocess(text):
    tokens = nltk.word_tokenize(text.lower())
    tokens = [lemmatizer.lemmatize(word) for word in tokens if word.isalpha() and word not in stop_words]
    return tokens
preprocessed_docs = [preprocess(doc) for doc in c_text4]
dictionary = corpora.Dictionary(preprocessed_docs)
corpus = [dictionary.doc2bow(doc) for doc in preprocessed_docs]

lda_model = gensim.models.LdaModel(corpus, num_topics=20, id2word=dictionary, passes=10)
topics = lda_model.print_topics(num_words=4)
for topic in topics:
    print(topic)

"""LDA does not seem to produce very relevant results"""

from sklearn.decomposition import LatentDirichletAllocation
def preprocess(text):
    stop_words = set(stopwords.words('english'))
    lemmatizer = WordNetLemmatizer()
    tokens = word_tokenize(text.lower())
    filtered_tokens = [lemmatizer.lemmatize(word) for word in tokens if word.isalnum() and word not in stop_words]
    return ' '.join(filtered_tokens)

# Preprocess the texts
preprocessed_texts = [preprocess(text) for text in c_text4]

# Create Bag of Words representation
vectorizer = CountVectorizer()
X = vectorizer.fit_transform(preprocessed_texts)

# Apply LDA
lda_model = LatentDirichletAllocation(n_components=10, random_state=42)
lda_output = lda_model.fit_transform(X)

# Visualize topics
def visualize_topics(model, feature_names, n_top_words):
    for topic_idx, topic in enumerate(model.components_):
        print(f"Topic {topic_idx + 1}:")
        top_features_idx = topic.argsort()[:-n_top_words - 1:-1]
        top_features = [feature_names[i] for i in top_features_idx]
        print(", ".join(top_features))
        wordcloud = WordCloud(background_color='white').generate(" ".join(top_features))
        plt.imshow(wordcloud, interpolation='bilinear')
        plt.axis("off")
        plt.show()

n_top_words = 10
feature_names = vectorizer.get_feature_names_out()
visualize_topics(lda_model, feature_names, n_top_words)


# Create TF-IDF table
tfidf_vectorizer = TfidfVectorizer()
X_tfidf = tfidf_vectorizer.fit_transform(preprocessed_texts)

# Visualize TF-IDF table
tfidf_df = pd.DataFrame(X_tfidf.toarray(), columns=tfidf_vectorizer.get_feature_names_out())
print(tfidf_df)

"""# GPT 4o Analysis and Insights

## 2024 and 2023 October

Insights from RBI's Monetary Policy Reports
a) Insights on the Trend RBI Presents
Economic Resilience and Growth:

The RBI highlights that despite global challenges, the Indian economy remains resilient. For the fiscal year 2023-24, India emerged as the fastest-growing major economy, bolstered by strong domestic demand, an upturn in the investment cycle, and a broad-based revival in manufacturing and services sectors .
Inflation Trends:

Headline inflation in India showed signs of moderation, primarily influenced by the volatility in food prices. While core inflation (excluding food and fuel) has been on a declining path, significant fluctuations in vegetable prices and other food items have caused short-term spikes in inflation .
Policy Measures and Rates:

The RBI maintained the policy repo rate at 6.50% through H2:2023-24, reflecting its commitment to align inflation with the target while supporting growth. The MPC's stance has been cautious, balancing between managing inflation and fostering economic growth .
b) RBI's Motivations
Anchoring Inflation Expectations:

A primary motivation for the RBI is to anchor inflation expectations to ensure price stability. This involves maintaining a steady policy rate and being prepared to implement measures that counteract inflationary pressures, particularly those driven by volatile food prices .
Supporting Economic Growth:

The RBI aims to support sustained economic growth by fostering a conducive environment for investment and consumption. This includes ensuring liquidity in the financial system and implementing measures that encourage domestic demand .
Mitigating Climate Risks:

The RBI is increasingly incorporating climate risks into its economic models. Recognizing the potential long-term impacts of climate change on economic stability and inflation, the RBI is motivated to address these risks through proactive monetary policy adjustments .
c) RBI's Take on Indian Economy and Its Future
Optimistic Growth Projections:

The RBI projects a robust growth outlook for India, with real GDP growth expected to be around 7.0% for the fiscal year 2024-25. This optimism is driven by strong domestic demand, a revival in investment cycles, and government capital expenditure initiatives .
Balancing Risks and Opportunities:

While the RBI acknowledges the risks posed by global economic uncertainties, it remains optimistic about India's growth trajectory. The focus is on leveraging domestic drivers of demand to mitigate external headwinds and sustain economic momentum .
Resilience Amidst Global Challenges:

The Indian economy's resilience is highlighted as a key strength. Despite global economic slowdowns and trade uncertainties, India's robust macroeconomic fundamentals and policy measures have positioned it as a leading growth economy globally .
d) Prominent Issues That Could Hamper India's Rapid Economic Growth
Climate Change and Environmental Risks:

Climate change poses significant risks to India's economic stability. The increasing frequency and intensity of extreme weather events can disrupt agricultural productivity, affect supply chains, and necessitate tighter monetary policies, which could hamper growth .
Volatility in Food Prices:

The volatility in food prices, driven by erratic rainfall and adverse weather conditions, presents a substantial challenge. Such volatility can lead to spikes in inflation, impacting overall economic stability and consumer confidence .
Global Economic Uncertainties:

Persistent uncertainties in the global economy, such as trade tensions and geopolitical risks, pose external threats to India's growth. These factors can impact exports, investment flows, and overall economic sentiment, necessitating careful monitoring and policy adjustments by the RBI .
Overall, the RBI's reports highlight a balanced approach towards managing inflation and fostering growth, while being cognizant of the challenges posed by climate change and global economic uncertainties. The emphasis is on maintaining economic resilience and leveraging domestic strengths to sustain growth in the face of external risks.

## Limit Reached!
"""

